% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kernelshap.R
\name{kernelshap}
\alias{kernelshap}
\title{Kernel SHAP}
\usage{
kernelshap(
  X,
  pred_fun,
  bg_X = X,
  bg_w = NULL,
  paired_sampling = TRUE,
  m = trunc(20 * sqrt(ncol(bg_X))),
  tol = 0.01,
  max_iter = 250,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{X}{Matrix or data.frame containing the observations to be explained.
Should only contain features required in \code{pred_fun}, i.e., it should only contain
features.}

\item{pred_fun}{A function taking objects like \code{X} as input and providing numeric
predictions. Example: If "fit" denotes a logistic regression fitted via \code{stats::glm},
and SHAP values should be on the probability scale, then this argument is
\code{function(X) predict(fit, X, type = "response")}.}

\item{bg_X}{Matrix or data.frame used as background dataset to calculate marginal
expectations. Its column structure must be similar to \code{X}.
If too large (>200 rows), use subsampling or some more sophisticated strategy.}

\item{bg_w}{Optional vector of case weights for each row of \code{bg_X}.}

\item{paired_sampling}{Logical flag indicating whether to use paired sampling.
The default is \code{TRUE}. This means that with every feature subset S,
also its complement is evaluated, which leads to faster convergence.}

\item{m}{Number of feature subsets S to be evaluated during one iteration.
By default \code{trunc(20 * sqrt(ncol(bg_X)))}. For the paired sampling strategy,
the actual number of evaluations is 2m.}

\item{tol}{Tolerance determining when to stop. The algorithm keeps sampling until
max(sigma_n) / diff(range(beta_n)) < tol, where sigma_n are the standard errors
and beta_n are the SHAP values of a given observation.}

\item{max_iter}{If the stopping criterion (see \code{tol}) is not reached after
\code{max_iter} iterations, then the algorithm stops.}

\item{verbose}{Set to \code{FALSE} to suppress messages and progress bar.}

\item{...}{Currently unused.}
}
\value{
An object of class "kernelshap" with the following components:
\itemize{
\item \code{S}: Matrix with SHAP values.
\item \code{X}: Same as parameter \code{X}.
\item \code{baseline}: The average prediction on the background data.
\item \code{SE}: Standard errors corresponding to \code{S}.
\item \code{n_iter}: Number of iterations until convergence per row.
\item \code{converged}: Logical vector indicating convergence per row.
}
}
\description{
This function implements the model-agnostic Kernel SHAP Algorithm 1
of Covert and Lee (2021) with or without paired sampling, see reference.
It is applied to each row in \code{X}. Due to its iterative nature,
standard errors of the resulting SHAP values are provided, and convergence is monitored.
During each iteration, \code{2m} feature subsets are evaluated,
until the worst standard error of the SHAP
values is small enough relative to the range of the SHAP values.
The data rows \code{X} to be explained and the
background data \code{bg_X} should only represent feature columns required by the
prediction function \code{pred_fun}. The latter is a function taking
a data structure like \code{X} or \code{bg_X} and provides one numeric
prediction per row.
}
\examples{
fit <- stats::lm(Sepal.Length ~ ., data = iris)
pred_fun <- function(X) stats::predict(fit, X)
s <- kernelshap(iris[1:2, -1], pred_fun = pred_fun, iris[-1])
s

# Matrix input works as well, and pred_fun may contain preprocessing steps.
fit <- stats::lm(Sepal.Length ~ ., data = iris[1:4])
pred_fun <- function(X) stats::predict(fit, as.data.frame(X))
X <- data.matrix(iris[2:4])
s <- kernelshap(X[1:3, ], pred_fun = pred_fun, X)
s
}
\references{
Ian Covert and Su-In Lee. Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, PMLR 130:3457-3465, 2021.
}
